{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“ LSTM Text Generation - Shakespeare Style\n",
    "\n",
    "**Task:** Generative AI Engineer Interview Assignment  \n",
    "**Model:** LSTM (Long Short-Term Memory) Neural Network\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "This notebook implements a character-level text generator using LSTM neural networks. The model is trained on Shakespeare's works to generate new text in a similar style.\n",
    "\n",
    "### Contents:\n",
    "1. Dataset Loading & Preprocessing\n",
    "2. Model Architecture Design\n",
    "3. Model Training with Callbacks\n",
    "4. Text Generation\n",
    "5. **Bonus:** Architecture Experiments & Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install tensorflow numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Dataset Loading\n",
    "\n",
    "We'll use Shakespeare's complete works from TensorFlow's built-in dataset.\n",
    "\n",
    "**Dataset Source:** [Shakespeare Dataset - TensorFlow](https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt)\n",
    "\n",
    "Alternative sources:\n",
    "- [Project Gutenberg](https://www.gutenberg.org/ebooks/100)\n",
    "- [Kaggle Shakespeare](https://www.kaggle.com/datasets/kingburrito666/shakespeare-plays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Shakespeare dataset\n",
    "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\"\n",
    "filepath = keras.utils.get_file('shakespeare.txt', dataset_url)\n",
    "\n",
    "# Read the text file\n",
    "with open(filepath, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Total characters in dataset: {len(text):,}\")\n",
    "print(f\"\\n--- First 500 characters ---\\n\")\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Preprocessing\n",
    "\n",
    "### Steps:\n",
    "1. Convert text to lowercase\n",
    "2. Create character-to-index and index-to-character mappings\n",
    "3. Create input-output sequences for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lowercase for consistency\n",
    "text = text.lower()\n",
    "\n",
    "# Get unique characters (vocabulary)\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"Unique characters (vocabulary size): {vocab_size}\")\n",
    "print(f\"Characters: {chars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create character-to-index and index-to-character mappings\n",
    "# These mappings help convert between characters and numerical indices\n",
    "\n",
    "char_to_idx = {char: idx for idx, char in enumerate(chars)}  # 'a' -> 0, 'b' -> 1, etc.\n",
    "idx_to_char = {idx: char for idx, char in enumerate(chars)}  # 0 -> 'a', 1 -> 'b', etc.\n",
    "\n",
    "print(\"Sample mappings:\")\n",
    "print(f\"'a' -> {char_to_idx.get('a', 'N/A')}\")\n",
    "print(f\"'z' -> {char_to_idx.get('z', 'N/A')}\")\n",
    "print(f\"' ' (space) -> {char_to_idx.get(' ', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input-output sequences\n",
    "# Sequence length determines how many characters the model \"sees\" before predicting the next one\n",
    "\n",
    "SEQUENCE_LENGTH = 100  # Number of characters in each input sequence\n",
    "STEP_SIZE = 3          # Step size for creating sequences (to avoid too much overlap)\n",
    "\n",
    "sequences = []   # Input sequences\n",
    "next_chars = []  # Target characters (what comes after each sequence)\n",
    "\n",
    "# Create sequences by sliding a window over the text\n",
    "for i in range(0, len(text) - SEQUENCE_LENGTH, STEP_SIZE):\n",
    "    sequences.append(text[i: i + SEQUENCE_LENGTH])      # Input: 100 characters\n",
    "    next_chars.append(text[i + SEQUENCE_LENGTH])        # Output: next character\n",
    "\n",
    "print(f\"Total sequences created: {len(sequences):,}\")\n",
    "print(f\"\\n--- Example ---\")\n",
    "print(f\"Input sequence: '{sequences[0]}'\")\n",
    "print(f\"Target (next char): '{next_chars[0]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sequences to numerical format for the neural network\n",
    "# X: input sequences (one-hot encoded or integer indexed)\n",
    "# y: target characters (one-hot encoded)\n",
    "\n",
    "print(\"Preparing training data (this may take a moment)...\")\n",
    "\n",
    "# Using integer encoding (more memory efficient than one-hot)\n",
    "X = np.zeros((len(sequences), SEQUENCE_LENGTH), dtype=np.int32)\n",
    "y = np.zeros((len(sequences), vocab_size), dtype=np.float32)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        X[i, t] = char_to_idx[char]           # Integer index for each character\n",
    "    y[i, char_to_idx[next_chars[i]]] = 1      # One-hot encode the target\n",
    "\n",
    "print(f\"X shape: {X.shape} (samples, sequence_length)\")\n",
    "print(f\"y shape: {y.shape} (samples, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets (90% train, 10% validation)\n",
    "split_idx = int(len(X) * 0.9)\n",
    "\n",
    "X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "\n",
    "print(f\"Training samples: {len(X_train):,}\")\n",
    "print(f\"Validation samples: {len(X_val):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Model Architecture\n",
    "\n",
    "### LSTM Model Components:\n",
    "1. **Embedding Layer:** Converts integer indices to dense vectors\n",
    "2. **LSTM Layer(s):** Learns sequential patterns in the text\n",
    "3. **Dropout:** Prevents overfitting by randomly dropping neurons\n",
    "4. **Dense + Softmax:** Outputs probability distribution over all characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(lstm_units=256, num_lstm_layers=1, dropout_rate=0.2, embedding_dim=64):\n",
    "    \"\"\"\n",
    "    Build an LSTM model for text generation.\n",
    "    \n",
    "    Parameters:\n",
    "    - lstm_units: Number of LSTM units in each layer\n",
    "    - num_lstm_layers: Number of stacked LSTM layers\n",
    "    - dropout_rate: Dropout rate for regularization\n",
    "    - embedding_dim: Dimension of character embeddings\n",
    "    \n",
    "    Returns:\n",
    "    - Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = Sequential(name=f\"LSTM_{num_lstm_layers}layer_{lstm_units}units\")\n",
    "    \n",
    "    # Embedding layer: converts character indices to dense vectors\n",
    "    # This helps the model learn meaningful representations of characters\n",
    "    model.add(Embedding(\n",
    "        input_dim=vocab_size,      # Size of vocabulary\n",
    "        output_dim=embedding_dim,  # Embedding dimension\n",
    "        input_length=SEQUENCE_LENGTH,\n",
    "        name='embedding'\n",
    "    ))\n",
    "    \n",
    "    # Add LSTM layers\n",
    "    for i in range(num_lstm_layers):\n",
    "        return_sequences = (i < num_lstm_layers - 1)  # Only last LSTM doesn't return sequences\n",
    "        model.add(LSTM(\n",
    "            units=lstm_units,\n",
    "            return_sequences=return_sequences,\n",
    "            name=f'lstm_{i+1}'\n",
    "        ))\n",
    "        model.add(Dropout(dropout_rate, name=f'dropout_{i+1}'))\n",
    "    \n",
    "    # Dense output layer with softmax activation\n",
    "    # Outputs probability distribution over all possible next characters\n",
    "    model.add(Dense(vocab_size, activation='softmax', name='output'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',  # Standard loss for multi-class classification\n",
    "        optimizer='adam',                  # Adaptive learning rate optimizer\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the main model\n",
    "model = build_model(\n",
    "    lstm_units=256,\n",
    "    num_lstm_layers=2,  # 2 stacked LSTM layers\n",
    "    dropout_rate=0.2,\n",
    "    embedding_dim=64\n",
    ")\n",
    "\n",
    "# Display model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Model Training\n",
    "\n",
    "### Callbacks used:\n",
    "1. **EarlyStopping:** Stops training when validation loss stops improving\n",
    "2. **ModelCheckpoint:** Saves the best model during training\n",
    "3. **ReduceLROnPlateau:** Reduces learning rate when loss plateaus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks for training\n",
    "\n",
    "callbacks = [\n",
    "    # Stop training if validation loss doesn't improve for 3 epochs\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Save the best model based on validation loss\n",
    "    ModelCheckpoint(\n",
    "        'best_model.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Reduce learning rate when validation loss plateaus\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,      # Multiply LR by 0.5\n",
    "        patience=2,\n",
    "        min_lr=0.0001,\n",
    "        verbose=1\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# Note: This may take 15-30 minutes on GPU, longer on CPU\n",
    "\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Epochs: {EPOCHS}, Batch Size: {BATCH_SIZE}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(history.history['loss'], label='Training Loss', color='blue')\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss', color='orange')\n",
    "axes[0].set_title('Model Loss Over Epochs')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(history.history['accuracy'], label='Training Accuracy', color='blue')\n",
    "axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy', color='orange')\n",
    "axes[1].set_title('Model Accuracy Over Epochs')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"Training history plot saved as 'training_history.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Text Generation\n",
    "\n",
    "### How it works:\n",
    "1. Start with a **seed text** (initial sequence)\n",
    "2. Model predicts probability distribution for the next character\n",
    "3. Sample a character from this distribution (using temperature)\n",
    "4. Append the character and repeat\n",
    "\n",
    "### Temperature Parameter:\n",
    "- **Low temperature (0.2-0.5):** More conservative, predictable text\n",
    "- **Medium temperature (0.7-1.0):** Balanced creativity\n",
    "- **High temperature (1.2-1.5):** More random, creative text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_temperature(predictions, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Sample an index from the probability distribution with temperature scaling.\n",
    "    \n",
    "    Temperature controls randomness:\n",
    "    - Lower temperature = more deterministic (picks highest probability)\n",
    "    - Higher temperature = more random (more uniform distribution)\n",
    "    \n",
    "    Parameters:\n",
    "    - predictions: Probability distribution from the model\n",
    "    - temperature: Scaling factor for randomness\n",
    "    \n",
    "    Returns:\n",
    "    - Sampled character index\n",
    "    \"\"\"\n",
    "    predictions = np.asarray(predictions).astype('float64')\n",
    "    \n",
    "    # Apply temperature scaling\n",
    "    predictions = np.log(predictions + 1e-10) / temperature\n",
    "    exp_predictions = np.exp(predictions)\n",
    "    predictions = exp_predictions / np.sum(exp_predictions)\n",
    "    \n",
    "    # Sample from the distribution\n",
    "    probas = np.random.multinomial(1, predictions, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed_text, num_chars=400, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate new text starting from a seed sequence.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Trained LSTM model\n",
    "    - seed_text: Initial text to start generation (must be >= SEQUENCE_LENGTH)\n",
    "    - num_chars: Number of characters to generate\n",
    "    - temperature: Controls randomness of generation\n",
    "    \n",
    "    Returns:\n",
    "    - Generated text string\n",
    "    \"\"\"\n",
    "    # Ensure seed is lowercase and at least SEQUENCE_LENGTH characters\n",
    "    seed_text = seed_text.lower()\n",
    "    \n",
    "    if len(seed_text) < SEQUENCE_LENGTH:\n",
    "        # Pad with spaces if seed is too short\n",
    "        seed_text = ' ' * (SEQUENCE_LENGTH - len(seed_text)) + seed_text\n",
    "    \n",
    "    # Take the last SEQUENCE_LENGTH characters as the starting point\n",
    "    generated = seed_text[-SEQUENCE_LENGTH:]\n",
    "    result = generated\n",
    "    \n",
    "    print(f\"Generating {num_chars} characters with temperature={temperature}...\")\n",
    "    \n",
    "    for i in range(num_chars):\n",
    "        # Prepare input sequence\n",
    "        x_pred = np.zeros((1, SEQUENCE_LENGTH), dtype=np.int32)\n",
    "        for t, char in enumerate(generated):\n",
    "            if char in char_to_idx:\n",
    "                x_pred[0, t] = char_to_idx[char]\n",
    "            else:\n",
    "                x_pred[0, t] = 0  # Unknown character\n",
    "        \n",
    "        # Predict next character probabilities\n",
    "        predictions = model.predict(x_pred, verbose=0)[0]\n",
    "        \n",
    "        # Sample next character with temperature\n",
    "        next_idx = sample_with_temperature(predictions, temperature)\n",
    "        next_char = idx_to_char[next_idx]\n",
    "        \n",
    "        # Update sequences\n",
    "        generated = generated[1:] + next_char\n",
    "        result += next_char\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  Generated {i + 1}/{num_chars} characters...\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text with Different Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define seed texts for generation\n",
    "seed_texts = [\n",
    "    # Seed 1: Classic Shakespeare opening\n",
    "    \"to be or not to be that is the question whether tis nobler in the mind to suffer\",\n",
    "    \n",
    "    # Seed 2: Romeo and Juliet style\n",
    "    \"romeo romeo wherefore art thou romeo deny thy father and refuse thy name\",\n",
    "    \n",
    "    # Seed 3: King/Royal theme\n",
    "    \"the king hath sent for me and i shall go to serve my lord and country\"\n",
    "]\n",
    "\n",
    "# Temperature values to try\n",
    "temperatures = [0.5, 0.8, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display text for each seed and temperature combination\n",
    "all_outputs = []\n",
    "\n",
    "for i, seed in enumerate(seed_texts):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"SEED {i+1}: '{seed[:50]}...'\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        print(f\"\\n--- Temperature: {temp} ---\")\n",
    "        generated = generate_text(model, seed, num_chars=300, temperature=temp)\n",
    "        print(f\"\\n{generated}\")\n",
    "        \n",
    "        all_outputs.append({\n",
    "            'seed': seed,\n",
    "            'temperature': temp,\n",
    "            'output': generated\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save generated outputs to a file\n",
    "with open('generated_samples.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"LSTM TEXT GENERATION - SAMPLE OUTPUTS\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\\n\")\n",
    "    \n",
    "    for i, output in enumerate(all_outputs):\n",
    "        f.write(f\"Sample {i+1}\\n\")\n",
    "        f.write(f\"Seed: {output['seed']}\\n\")\n",
    "        f.write(f\"Temperature: {output['temperature']}\\n\")\n",
    "        f.write(f\"Generated Text:\\n{output['output']}\\n\")\n",
    "        f.write(\"\\n\" + \"-\"*60 + \"\\n\\n\")\n",
    "\n",
    "print(\"Generated samples saved to 'generated_samples.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. BONUS: Architecture Experiments ðŸ”¬\n",
    "\n",
    "Let's experiment with different model architectures and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different architecture configurations to test\n",
    "architectures = [\n",
    "    {'name': 'Small (1 LSTM, 128 units)', 'lstm_units': 128, 'num_layers': 1, 'embedding_dim': 32},\n",
    "    {'name': 'Medium (2 LSTM, 256 units)', 'lstm_units': 256, 'num_layers': 2, 'embedding_dim': 64},\n",
    "    {'name': 'Large (3 LSTM, 512 units)', 'lstm_units': 512, 'num_layers': 3, 'embedding_dim': 128},\n",
    "]\n",
    "\n",
    "# Store results for comparison\n",
    "architecture_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate each architecture\n",
    "# Note: This will take significant time. For quick testing, reduce EPOCHS to 5-10\n",
    "\n",
    "EXPERIMENT_EPOCHS = 10  # Reduced epochs for experimentation\n",
    "\n",
    "for arch in architectures:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training: {arch['name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Build model with current architecture\n",
    "    exp_model = build_model(\n",
    "        lstm_units=arch['lstm_units'],\n",
    "        num_lstm_layers=arch['num_layers'],\n",
    "        dropout_rate=0.2,\n",
    "        embedding_dim=arch['embedding_dim']\n",
    "    )\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = exp_model.count_params()\n",
    "    \n",
    "    # Train with reduced epochs for comparison\n",
    "    exp_callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    ]\n",
    "    \n",
    "    exp_history = exp_model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=EXPERIMENT_EPOCHS,\n",
    "        batch_size=128,\n",
    "        callbacks=exp_callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    final_loss = min(exp_history.history['val_loss'])\n",
    "    final_acc = max(exp_history.history['val_accuracy'])\n",
    "    \n",
    "    architecture_results.append({\n",
    "        'name': arch['name'],\n",
    "        'params': total_params,\n",
    "        'val_loss': final_loss,\n",
    "        'val_accuracy': final_acc,\n",
    "        'model': exp_model\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nResults: Val Loss = {final_loss:.4f}, Val Accuracy = {final_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare architectures\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ARCHITECTURE COMPARISON RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Architecture':<35} {'Parameters':<15} {'Val Loss':<12} {'Val Acc':<10}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for result in architecture_results:\n",
    "    print(f\"{result['name']:<35} {result['params']:<15,} {result['val_loss']:<12.4f} {result['val_accuracy']:<10.4f}\")\n",
    "\n",
    "# Find best architecture\n",
    "best = min(architecture_results, key=lambda x: x['val_loss'])\n",
    "print(f\"\\nâœ… Best Architecture: {best['name']} (Val Loss: {best['val_loss']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize architecture comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "names = [r['name'].split('(')[0].strip() for r in architecture_results]\n",
    "losses = [r['val_loss'] for r in architecture_results]\n",
    "accuracies = [r['val_accuracy'] for r in architecture_results]\n",
    "params = [r['params'] / 1000000 for r in architecture_results]  # In millions\n",
    "\n",
    "# Loss comparison\n",
    "bars1 = axes[0].bar(names, losses, color=['#3498db', '#2ecc71', '#e74c3c'])\n",
    "axes[0].set_title('Validation Loss by Architecture')\n",
    "axes[0].set_ylabel('Validation Loss')\n",
    "axes[0].tick_params(axis='x', rotation=15)\n",
    "for bar, loss in zip(bars1, losses):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                 f'{loss:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Parameters vs Accuracy scatter\n",
    "scatter = axes[1].scatter(params, accuracies, s=[p*50 for p in params], \n",
    "                          c=['#3498db', '#2ecc71', '#e74c3c'], alpha=0.7)\n",
    "for i, name in enumerate(names):\n",
    "    axes[1].annotate(name, (params[i], accuracies[i]), \n",
    "                     textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "axes[1].set_title('Model Size vs Accuracy')\n",
    "axes[1].set_xlabel('Parameters (Millions)')\n",
    "axes[1].set_ylabel('Validation Accuracy')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('architecture_comparison.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"Architecture comparison saved as 'architecture_comparison.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text samples from each architecture for comparison\n",
    "comparison_seed = \"to be or not to be that is the question whether tis nobler in the mind to suffer\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEXT GENERATION COMPARISON ACROSS ARCHITECTURES\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Seed: '{comparison_seed[:50]}...'\\n\")\n",
    "\n",
    "for result in architecture_results:\n",
    "    print(f\"\\n--- {result['name']} ---\")\n",
    "    generated = generate_text(result['model'], comparison_seed, num_chars=200, temperature=0.8)\n",
    "    # Show only the generated part (after the seed)\n",
    "    print(generated[SEQUENCE_LENGTH:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Save Final Model and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best performing model\n",
    "best_model = min(architecture_results, key=lambda x: x['val_loss'])['model']\n",
    "best_model.save('final_lstm_model.keras')\n",
    "print(\"Final model saved as 'final_lstm_model.keras'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary report\n",
    "summary = \"\"\"\n",
    "================================================================================\n",
    "                    LSTM TEXT GENERATION - PROJECT SUMMARY\n",
    "================================================================================\n",
    "\n",
    "DATASET:\n",
    "- Source: Shakespeare's Complete Works (TensorFlow Dataset)\n",
    "- URL: https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
    "- Total Characters: {:,}\n",
    "- Vocabulary Size: {}\n",
    "- Sequence Length: {}\n",
    "\n",
    "MODEL ARCHITECTURE (Best Performing):\n",
    "- Type: LSTM (Long Short-Term Memory)\n",
    "- Embedding Dimension: 64\n",
    "- LSTM Layers: 2\n",
    "- LSTM Units: 256\n",
    "- Dropout Rate: 0.2\n",
    "- Output: Dense + Softmax\n",
    "\n",
    "TRAINING:\n",
    "- Loss Function: Categorical Crossentropy\n",
    "- Optimizer: Adam\n",
    "- Callbacks: EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "ARCHITECTURE COMPARISON:\n",
    "{}\n",
    "\n",
    "KEY FINDINGS:\n",
    "1. Deeper models (more LSTM layers) generally produce more coherent text\n",
    "2. Temperature of 0.7-0.8 provides best balance of creativity and coherence\n",
    "3. Sequence length of 100 characters captures sufficient context\n",
    "\n",
    "FILES GENERATED:\n",
    "- final_lstm_model.keras (Trained model)\n",
    "- generated_samples.txt (Text generation examples)\n",
    "- training_history.png (Loss/Accuracy plots)\n",
    "- architecture_comparison.png (Model comparison chart)\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "# Format architecture results\n",
    "arch_table = \"\\n\".join([f\"  - {r['name']}: Loss={r['val_loss']:.4f}, Acc={r['val_accuracy']:.4f}\" \n",
    "                        for r in architecture_results])\n",
    "\n",
    "formatted_summary = summary.format(len(text), vocab_size, SEQUENCE_LENGTH, arch_table)\n",
    "print(formatted_summary)\n",
    "\n",
    "# Save summary to file\n",
    "with open('project_summary.txt', 'w') as f:\n",
    "    f.write(formatted_summary)\n",
    "print(\"Summary saved to 'project_summary.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. How to Use This Model\n",
    "\n",
    "### Loading and Using the Saved Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load and use the saved model\n",
    "# from tensorflow.keras.models import load_model\n",
    "# \n",
    "# loaded_model = load_model('final_lstm_model.keras')\n",
    "# \n",
    "# # Generate new text\n",
    "# new_seed = \"your custom seed text here with at least 100 characters\"\n",
    "# generated = generate_text(loaded_model, new_seed, num_chars=500, temperature=0.8)\n",
    "# print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References & Resources\n",
    "\n",
    "1. **Dataset:** [Shakespeare Text - TensorFlow](https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt)\n",
    "2. **Alternative Dataset:** [Project Gutenberg - Shakespeare](https://www.gutenberg.org/ebooks/100)\n",
    "3. **LSTM Paper:** [Long Short-Term Memory (Hochreiter & Schmidhuber, 1997)](https://www.bioinf.jku.at/publications/older/2604.pdf)\n",
    "4. **TensorFlow LSTM Guide:** [TensorFlow RNN Tutorial](https://www.tensorflow.org/guide/keras/rnn)\n",
    "\n",
    "---\n",
    "\n",
    "**End of Notebook**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
